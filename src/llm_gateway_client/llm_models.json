[
    {
        "name": "meta-llama/Llama-2-7b-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "meta-llama/Llama-2-13b-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "TheBloke/Llama-2-70B-GGUF",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf-split-*"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "meta-llama/Llama-2-7b-chat-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1027,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "meta-llama/Llama-2-13b-chat-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1045,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "TheBloke/Llama-2-70B-Chat-GGUF",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1082,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf-split-*"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "codellama/CodeLlama-7b-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": false,
        "infill": true,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "codellama/CodeLlama-13b-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": false,
        "infill": true,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "codellama/CodeLlama-34b-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": false,
        "infill": true,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": "TheBloke/CodeLlama-34B-GGUF"
    },
    {
        "name": "TheBloke/CodeLlama-34B-GGUF",
        "engine": "llama.cpp",
        "template": "llama2_style_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": false,
        "infill": true,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "codellama/CodeLlama-7b-Instruct-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "codellama/CodeLlama-13b-Instruct-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "codellama/CodeLlama-34b-Instruct-hf",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1043,
        "expert": "code",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": "TheBloke/CodeLlama-34B-Instruct-GGUF"
    },
    {
        "name": "TheBloke/CodeLlama-34B-Instruct-GGUF",
        "engine": "llama.cpp",
        "template": "llama2_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1043,
        "expert": "code",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 100000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "meta-llama/Meta-Llama-3-8B-Instruct",
        "engine": "llama.cpp",
        "template": "llama3_style_instruct_prompt_template",
        "gguf_script": "hf",
        "revision": null,
        "score": 1154,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 8192,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "QuantFactory/Meta-Llama-3-70B-Instruct-GGUF",
        "engine": "llama.cpp",
        "template": "llama3_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1208,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 8192,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0-000??-of-000??.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "bigcode/starcoder",
        "engine": "llama.cpp",
        "template": "starcoder_style_completion_template",
        "gguf_script": "hf",
        "revision": null,
        "score": 0,
        "expert": "code",
        "chat": false,
        "infill": true,
        "price": 0,
        "max_context": 8000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.bin"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "TheBloke/Mistral-7B-v0.1-GGUF",
        "engine": "llama.cpp",
        "template": "default_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 8192,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "TheBloke/Mistral-7B-Instruct-v0.1-GGUF",
        "engine": "llama.cpp",
        "template": "mistral_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1106,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 8192,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "TheBloke/Mixtral-8x7B-v0.1-GGUF",
        "engine": "llama.cpp",
        "template": "default_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 32768,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
        "engine": "llama.cpp",
        "template": "mistral_style_instruct_prompt_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1120,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 32768,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.Q8_0.gguf"
        ],
        "quantized": true,
        "quant_alternative": null
    },
    {
        "name": "01-ai/Yi-6B",
        "engine": "llama.cpp",
        "template": "default_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "01-ai/Yi-6B-200K",
        "engine": "llama.cpp",
        "template": "default_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 819200,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "01-ai/Yi-34B",
        "engine": "llama.cpp",
        "template": "default_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "01-ai/Yi-34B-200K",
        "engine": "llama.cpp",
        "template": "default_completion_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": false,
        "infill": false,
        "price": 0,
        "max_context": 819200,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "01-ai/Yi-6B-Chat",
        "engine": "llama.cpp",
        "template": "yi_style_chat_template",
        "gguf_script": "default",
        "revision": null,
        "score": 0,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "01-ai/Yi-34B-Chat",
        "engine": "llama.cpp",
        "template": "yi_style_chat_template",
        "gguf_script": "default",
        "revision": null,
        "score": 1111,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 4096,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "microsoft/Phi-3-mini-128k-instruct",
        "engine": "llama.cpp",
        "template": "phi3_style_instruct_template",
        "gguf_script": "hf",
        "revision": null,
        "score": 1052,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 0,
        "max_context": 131072,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.safetensors"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "Salesforce/codegen25-7b-multi",
        "engine": "transformers",
        "template": "codegen_style_completion_template",
        "revision": "f319d912c0c73ea3682094202b209ac8cb5d4cba",
        "score": 0,
        "expert": "code",
        "chat": false,
        "infill": true,
        "price": 0,
        "max_context": 2000,
        "file_list": [
            ".gitattributes",
            "*.json",
            "*.model",
            "*.md",
            "*.txt",
            "*.bin"
        ],
        "quantized": false,
        "quant_alternative": null
    },
    {
        "name": "gpt-4o",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 15.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4o-2024-05-13",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 15.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4-turbo",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 30.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4-turbo-2024-04-09",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 30.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4-0125-preview",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 30.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4-1106-preview",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 30.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4-turbo-preview",
        "engine": "openai",
        "score": 1251,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 30.0,
        "max_context": 128000
    },
    {
        "name": "gpt-4",
        "engine": "openai",
        "score": 1161,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 60.0,
        "max_context": 8192
    },
    {
        "name": "gpt-4-32k",
        "engine": "openai",
        "score": 1161,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 120.0,
        "max_context": 32768
    },
    {
        "name": "gpt-3.5-turbo",
        "engine": "openai",
        "score": 1097,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 1.5,
        "max_context": 16385
    },
    {
        "name": "claude-3-opus-20240229",
        "engine": "anthropic",
        "score": 1233,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 75.0,
        "max_context": 204800
    },
    {
        "name": "claude-3-sonnet-20240229",
        "engine": "anthropic",
        "score": 1180,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 15.0,
        "max_context": 204800
    },
    {
        "name": "claude-3-haiku-20240307",
        "engine": "anthropic",
        "score": 1097,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 1.25,
        "max_context": 204800
    },
    {
        "name": "claude-2.1",
        "engine": "anthropic",
        "score": 1116,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 24.0,
        "max_context": 102400
    },
    {
        "name": "claude-2.0",
        "engine": "anthropic",
        "score": 1127,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 24.0,
        "max_context": 102400
    },
    {
        "name": "claude-instant-1.2",
        "engine": "anthropic",
        "score": 1105,
        "expert": "general",
        "chat": true,
        "infill": false,
        "price": 2.4,
        "max_context": 102400
    }
]